{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typeform: ML Case (3): PySpark\n",
    "\n",
    "Run this notebook on an standalone spark on the root folder and feed it with the \"typeform.csv\" data file.\n",
    "\n",
    "note: an error occurs while fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typeform: PySpark: Reading the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"./typeform.csv\", format=\"text\")\\\n",
    "    .withColumn('form_id', f.split('value', ',')[0])\\\n",
    "    .withColumn('submissions', f.split('value', ',')[1].cast(IntegerType()))\\\n",
    "    .withColumn('views', f.split('value', ',')[2].cast(IntegerType()))\\\n",
    "    .withColumn('features', f.split('value', ',')[3])\\\n",
    "    .select('form_id','submissions','views','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(form_id=1113027, submissions=33, views=27, completion_rate=1.2222222222222223, features='0.0-0.0-0.0-0.0-0.0-0.0-1.0-0.0-1.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-2.0-0.0-2.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-2.0-1.0-2.0'),\n",
       " Row(form_id=1115313, submissions=147, views=111, completion_rate=1.3243243243243243, features='0.0-2.0-0.0-0.0-0.0-0.0-0.0-0.0-1.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-3.0-0.0-3.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0-0.0')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rdd.map(lambda x : (\n",
    "    int(x.form_id[1:]),\n",
    "    x.submissions,\n",
    "    x.views,\n",
    "    x.submissions / x.views if x.views > 0.0 else 0.0,\n",
    "    x.features[:-1]\n",
    ")).toDF()\\\n",
    "    .withColumnRenamed(\"_1\",\"form_id\")\\\n",
    "    .withColumnRenamed(\"_2\",\"submissions\")\\\n",
    "    .withColumnRenamed(\"_3\",\"views\")\\\n",
    "    .withColumnRenamed(\"_4\",\"completion_rate\")\\\n",
    "    .withColumnRenamed(\"_5\",\"features\")\n",
    "\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- form_id: long (nullable = true)\n",
      " |-- submissions: long (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      " |-- completion_rate: double (nullable = true)\n",
      " |-- features: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typeform: PySpark: Features clean and unstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(df.views > 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "   'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n",
    "   'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n",
    "   'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n",
    "   'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n",
    "   'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24',\n",
    "   'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n",
    "   'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n",
    "   'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39',\n",
    "   'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44',\n",
    "   'feature_45', 'feature_46']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = f.split(df['features'], '-')\n",
    "for i in range(len(features)):\n",
    "    df = df.withColumn(features[i], split_col.getItem(i).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.select('completion_rate',\n",
    "    'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n",
    "    'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n",
    "    'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n",
    "    'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n",
    "    'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24',\n",
    "    'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n",
    "    'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n",
    "    'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39',\n",
    "    'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44',\n",
    "    'feature_45', 'feature_46')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typeform: PySpark: Features VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df_ = df_.where(df_.completion_rate.isNotNull())\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features,outputCol=\"features\")\n",
    "\n",
    "df_assembler = assembler.setHandleInvalid(\"skip\").transform(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, df_validate = df_assembler.randomSplit([0.6, 0.35, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typeform: PySpark: Model training linear regression\n",
    "\n",
    "note: Py4JJavaError: An error occurred while calling o389.fit.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 4.0 failed 1 times, most recent failure: Lost task 3.0 in stage 4.0 (TID 7, localhost, executor driver): java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol='features', labelCol='completion_rate')\n",
    "lr_model = lr.fit(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typeform: PySpark: Output statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"R2: %f\" % trainingSummary.r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
