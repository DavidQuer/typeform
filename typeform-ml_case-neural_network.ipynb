{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typeform: ML Case: Deep Neural Network\n",
    "\n",
    "refs[0]: https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typeform: ML Case: DNN : Read dataframe\n",
    "\n",
    "We are using the same dataframe cleaned in the python-sklearn exercise without eliminating any feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typeform = pd.read_pickle('./data/df_typeform.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1031283, 48)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_typeform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typeform: ML Case: DNN : Training set and Test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_typeform[['completion_rate']]\n",
    "X = df_typeform[list(df_typeform.columns[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690959, 47)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(340324, 47)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typeform: DNN: Model creation and fitting\n",
    "- Define a sequential model\n",
    "- Add some dense layers\n",
    "- Use ‘relu’ as the activation function for the hidden layers\n",
    "- Use a ‘normal’ initializer as the kernal_intializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the DNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1012 19:47:39.415066 4582729152 deprecation_wrapper.py:119] From /Users/davidquer/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1012 19:47:39.435066 4582729152 deprecation_wrapper.py:119] From /Users/davidquer/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1012 19:47:39.438317 4582729152 deprecation_wrapper.py:119] From /Users/davidquer/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W1012 19:47:39.595732 4582729152 deprecation_wrapper.py:119] From /Users/davidquer/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               6144      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 171,009\n",
      "Trainable params: 171,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DNN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "DNN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "DNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "DNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "DNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "DNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "DNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "DNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model fitting and checkpoints definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = './checkpoints/Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only 10 epochs so it ends in a reasonable time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1012 19:48:36.837498 4582729152 deprecation_wrapper.py:119] From /Users/davidquer/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W1012 19:48:36.952099 4582729152 deprecation_wrapper.py:119] From /Users/davidquer/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 552767 samples, validate on 138192 samples\n",
      "Epoch 1/10\n",
      "552767/552767 [==============================] - 24s 44us/step - loss: 2.1486 - mean_absolute_error: 2.1486 - val_loss: 2.1403 - val_mean_absolute_error: 2.1403\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.14035, saving model to ./checkpoints/Weights-001--2.14035.hdf5\n",
      "Epoch 2/10\n",
      "552767/552767 [==============================] - 20s 36us/step - loss: 2.1393 - mean_absolute_error: 2.1393 - val_loss: 2.1372 - val_mean_absolute_error: 2.1372\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.14035 to 2.13722, saving model to ./checkpoints/Weights-002--2.13722.hdf5\n",
      "Epoch 3/10\n",
      "552767/552767 [==============================] - 20s 36us/step - loss: 2.1375 - mean_absolute_error: 2.1375 - val_loss: 2.1370 - val_mean_absolute_error: 2.1370\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.13722 to 2.13704, saving model to ./checkpoints/Weights-003--2.13704.hdf5\n",
      "Epoch 4/10\n",
      "552767/552767 [==============================] - 20s 36us/step - loss: 2.1359 - mean_absolute_error: 2.1359 - val_loss: 2.1366 - val_mean_absolute_error: 2.1366\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.13704 to 2.13658, saving model to ./checkpoints/Weights-004--2.13658.hdf5\n",
      "Epoch 5/10\n",
      "552767/552767 [==============================] - 20s 36us/step - loss: 2.1348 - mean_absolute_error: 2.1348 - val_loss: 2.1349 - val_mean_absolute_error: 2.1349\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.13658 to 2.13493, saving model to ./checkpoints/Weights-005--2.13493.hdf5\n",
      "Epoch 6/10\n",
      "552767/552767 [==============================] - 20s 36us/step - loss: 2.1336 - mean_absolute_error: 2.1336 - val_loss: 2.1342 - val_mean_absolute_error: 2.1342\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.13493 to 2.13423, saving model to ./checkpoints/Weights-006--2.13423.hdf5\n",
      "Epoch 7/10\n",
      "552767/552767 [==============================] - 20s 36us/step - loss: 2.1330 - mean_absolute_error: 2.1330 - val_loss: 2.1341 - val_mean_absolute_error: 2.1341\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.13423 to 2.13413, saving model to ./checkpoints/Weights-007--2.13413.hdf5\n",
      "Epoch 8/10\n",
      "552767/552767 [==============================] - 23s 42us/step - loss: 2.1320 - mean_absolute_error: 2.1320 - val_loss: 2.1369 - val_mean_absolute_error: 2.1369\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.13413\n",
      "Epoch 9/10\n",
      "552767/552767 [==============================] - 23s 42us/step - loss: 2.1315 - mean_absolute_error: 2.1315 - val_loss: 2.1321 - val_mean_absolute_error: 2.1321\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.13413 to 2.13214, saving model to ./checkpoints/Weights-009--2.13214.hdf5\n",
      "Epoch 10/10\n",
      "552767/552767 [==============================] - 26s 46us/step - loss: 2.1305 - mean_absolute_error: 2.1305 - val_loss: 2.1338 - val_mean_absolute_error: 2.1338\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.13214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a477ffef0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting the best model based on MAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wights_file = './checkpoints/Weights-009--2.13214.hdf5'\n",
    "\n",
    "DNN_model.load_weights(wights_file)\n",
    "DNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model with the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = DNN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.columns = ['y_test','y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1487201872250994"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test['MAE'] = y_test.y_test - y_test.y_pred\n",
    "y_test['MAE'] = y_test.MAE.apply(lambda x : np.absolute(x))\n",
    "y_test.MAE.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
